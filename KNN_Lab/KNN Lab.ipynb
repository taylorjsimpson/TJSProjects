{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4, Lab 1: Predicting Left-Handedness from Psychological Factors\n",
    "> Author: Matt Brems\n",
    "\n",
    "We can sketch out the data science process as follows:\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "We'll walk through a full data science problem in this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Define The Problem.\n",
    "\n",
    "You're currently a data scientist working at a university. A professor of psychology is attempting to study the relationship between personalities and left-handedness. They have tasked you with gathering evidence so that they may publish.\n",
    "\n",
    "Specifically, the professor says \"I need to prove that left-handedness is caused by some personality trait. Go find that personality trait and the data to back it up.\"\n",
    "\n",
    "As a data scientist, you know that any real data science problem must be **specific** and **conclusively answerable**. For example:\n",
    "- Bad data science problem: \"What is the link between obesity and blood pressure?\"\n",
    "    - This is vague and is not conclusively answerable. That is, two people might look at the conclusion and one may say \"Sure, the problem has been answered!\" and the other may say \"The problem has not yet been answered.\"\n",
    "- Good data science problem: \"Does an association exist between obesity and blood pressure?\"\n",
    "    - This is more specific and is conclusively answerable. The problem specifically is asking for a \"Yes\" or \"No\" answer. Based on that, two independent people should both be able to say either \"Yes, the problem has been answered\" or \"No, the problem has not yet been answered.\"\n",
    "- Excellent data science problem: \"As obesity increases, how does blood pressure change?\"\n",
    "    - This is very specific and is conclusively answerable. The problem specifically seeks to understand the effect of one variable on the other.\n",
    "\n",
    "### 1. In the context of the left-handedness and personality example, what are three specific and conclusively answerable problems that you could answer using data science? \n",
    "\n",
    "> You might find it helpful to check out the codebook in the repo for some inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1. Does an association exist between dancing when alone and being left handed?\n",
    "2. Does an association exist between hating shopping and being left handed?\n",
    "3. Does an association exist between playing video games and being left handed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Obtain the data.\n",
    "\n",
    "### 2. Read in the file titled \"data.csv.\"\n",
    "> Hint: Despite being saved as a .csv file, you won't be able to simply `pd.read_csv()` this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table(\"C:\\\\Users\\\\taylo\\\\Desktop\\\\GA\\\\DSI-Assignments\\\\4.01-lab-classification_model_comparison\\\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NL</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  country  fromgoogle  engnat  \\\n",
       "0   4   1   5   1   5   1   5   1   4    1  ...       US           2       1   \n",
       "1   1   5   1   4   2   5   5   4   1    5  ...       CA           2       1   \n",
       "2   1   2   1   1   5   4   3   2   1    4  ...       NL           2       2   \n",
       "3   1   4   1   5   1   4   5   4   3    5  ...       US           2       1   \n",
       "4   5   1   5   1   5   1   5   1   3    1  ...       US           2       1   \n",
       "\n",
       "   age  education  gender  orientation  race  religion  hand  \n",
       "0   22          3       1            1     3         2     3  \n",
       "1   14          1       2            2     6         1     1  \n",
       "2   30          4       1            1     1         1     2  \n",
       "3   18          2       2            5     3         2     2  \n",
       "4   22          3       1            1     3         2     3  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1              int64\n",
       "Q2              int64\n",
       "Q3              int64\n",
       "Q4              int64\n",
       "Q5              int64\n",
       "Q6              int64\n",
       "Q7              int64\n",
       "Q8              int64\n",
       "Q9              int64\n",
       "Q10             int64\n",
       "Q11             int64\n",
       "Q12             int64\n",
       "Q13             int64\n",
       "Q14             int64\n",
       "Q15             int64\n",
       "Q16             int64\n",
       "Q17             int64\n",
       "Q18             int64\n",
       "Q19             int64\n",
       "Q20             int64\n",
       "Q21             int64\n",
       "Q22             int64\n",
       "Q23             int64\n",
       "Q24             int64\n",
       "Q25             int64\n",
       "Q26             int64\n",
       "Q27             int64\n",
       "Q28             int64\n",
       "Q29             int64\n",
       "Q30             int64\n",
       "Q31             int64\n",
       "Q32             int64\n",
       "Q33             int64\n",
       "Q34             int64\n",
       "Q35             int64\n",
       "Q36             int64\n",
       "Q37             int64\n",
       "Q38             int64\n",
       "Q39             int64\n",
       "Q40             int64\n",
       "Q41             int64\n",
       "Q42             int64\n",
       "Q43             int64\n",
       "Q44             int64\n",
       "introelapse     int64\n",
       "testelapse      int64\n",
       "country        object\n",
       "fromgoogle      int64\n",
       "engnat          int64\n",
       "age             int64\n",
       "education       int64\n",
       "gender          int64\n",
       "orientation     int64\n",
       "race            int64\n",
       "religion        int64\n",
       "hand            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
       "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
       "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
       "       'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40', 'Q41',\n",
       "       'Q42', 'Q43', 'Q44', 'introelapse', 'testelapse', 'country',\n",
       "       'fromgoogle', 'engnat', 'age', 'education', 'gender', 'orientation',\n",
       "       'race', 'religion', 'hand'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.isnull of       Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  country  fromgoogle  \\\n",
       "0      4   1   5   1   5   1   5   1   4    1  ...       US           2   \n",
       "1      1   5   1   4   2   5   5   4   1    5  ...       CA           2   \n",
       "2      1   2   1   1   5   4   3   2   1    4  ...       NL           2   \n",
       "3      1   4   1   5   1   4   5   4   3    5  ...       US           2   \n",
       "4      5   1   5   1   5   1   5   1   3    1  ...       US           2   \n",
       "5      5   4   2   2   1   1   3   3   3    1  ...       US           2   \n",
       "6      3   4   4   4   1   4   3   5   5    4  ...       GR           2   \n",
       "7      1   2   1   1   1   4   1   2   5    3  ...       GR           2   \n",
       "8      1   5   1   2   2   4   3   2   2    3  ...       GR           2   \n",
       "9      1   4   3   1   1   3   5   2   2    5  ...       US           2   \n",
       "10     3   2   3   1   2   4   4   1   5    4  ...       GB           2   \n",
       "11     1   1   1   1   5   3   3   2   1    1  ...       US           2   \n",
       "12     3   5   1   4   4   4   5   3   3    3  ...       US           2   \n",
       "13     1   1   5   3   3   5   5   5   1    3  ...       US           2   \n",
       "14     1   5   5   4   1   5   4   5   3    2  ...       US           2   \n",
       "15     1   5   1   3   3   5   1   3   3    3  ...       US           2   \n",
       "16     1   2   2   1   1   3   1   1   5    4  ...       KR           2   \n",
       "17     1   4   1   3   1   4   4   5   1    2  ...       US           2   \n",
       "18     4   5   3   4   5   4   4   2   5    4  ...       SE           2   \n",
       "19     1   5   1   1   1   5   1   2   1    4  ...       NO           2   \n",
       "20     1   5   1   4   2   2   2   4   4    5  ...       US           2   \n",
       "21     2   5   4   3   1   4   5   3   2    3  ...       DE           2   \n",
       "22     1   5   2   4   4   4   1   4   1    4  ...       CA           2   \n",
       "23     1   5   5   5   1   4   5   5   1    3  ...       DE           2   \n",
       "24     1   5   1   4   2   4   5   3   1    2  ...       US           2   \n",
       "25     1   5   5   5   1   5   1   5   1    5  ...       NZ           2   \n",
       "26     1   5   1   2   1   2   2   4   1    3  ...       US           2   \n",
       "27     1   4   1   3   1   1   1   3   1    2  ...       US           2   \n",
       "28     1   5   1   5   1   1   1   5   2    4  ...       US           2   \n",
       "29     1   5   5   2   4   5   1   3   2    3  ...       US           2   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...      ...         ...   \n",
       "4154   1   4   1   2   3   5   4   1   1    5  ...       CA           1   \n",
       "4155   1   5   5   5   2   5   5   3   4    3  ...       SA           1   \n",
       "4156   2   1   3   2   3   5   5   1   4    0  ...       US           2   \n",
       "4157   4   4   4   4   5   5   3   5   5    5  ...       FR           2   \n",
       "4158   3   5   4   2   1   4   2   3   4    5  ...       US           1   \n",
       "4159   1   1   3   5   1   0   3   3   3    1  ...       US           2   \n",
       "4160   1   5   1   1   1   5   3   3   5    5  ...       US           2   \n",
       "4161   5   2   4   1   1   1   5   1   3    1  ...       DE           1   \n",
       "4162   1   5   4   4   5   5   5   3   1    3  ...       US           1   \n",
       "4163   1   0   2   5   5   2   5   4   4    5  ...       MY           1   \n",
       "4164   1   4   4   2   5   5   2   5   4    4  ...       GB           1   \n",
       "4165   4   5   2   5   2   5   1   3   1    4  ...       DE           2   \n",
       "4166   2   5   5   5   3   2   3   1   4    4  ...       US           2   \n",
       "4167   1   3   1   2   1   3   2   3   4    3  ...       US           1   \n",
       "4168   1   3   4   3   2   3   1   1   3    5  ...       EG           2   \n",
       "4169   5   2   5   3   4   2   4   3   4    5  ...       CA           2   \n",
       "4170   1   2   2   5   5   3   2   2   1    3  ...       US           1   \n",
       "4171   1   2   1   1   1   2   1   1   1    3  ...       JO           2   \n",
       "4172   2   5   4   3   5   5   2   2   3    5  ...       CH           2   \n",
       "4173   1   4   1   3   2   5   1   5   1    4  ...       PL           1   \n",
       "4174   1   5   3   4   5   4   3   2   1    5  ...       US           2   \n",
       "4175   1   4   1   1   4   5   4   3   1    4  ...       EG           2   \n",
       "4176   1   1   5   5   1   5   5   3   5    5  ...       PL           2   \n",
       "4177   1   3   4   3   3   5   5   2   1    5  ...       IE           1   \n",
       "4178   1   2   1   1   3   4   4   1   1    2  ...       US           1   \n",
       "4179   3   5   4   5   2   4   2   2   2    5  ...       US           1   \n",
       "4180   1   5   1   5   1   4   2   4   1    4  ...       US           1   \n",
       "4181   3   2   2   4   5   4   5   2   2    5  ...       PL           2   \n",
       "4182   1   3   4   5   1   3   3   1   1    3  ...       US           2   \n",
       "4183   2   5   3   3   5   3   4   3   1    5  ...       NZ           2   \n",
       "\n",
       "      engnat  age  education  gender  orientation  race  religion  hand  \n",
       "0          1   22          3       1            1     3         2     3  \n",
       "1          1   14          1       2            2     6         1     1  \n",
       "2          2   30          4       1            1     1         1     2  \n",
       "3          1   18          2       2            5     3         2     2  \n",
       "4          1   22          3       1            1     3         2     3  \n",
       "5          1   59          4       1            1     6         7     1  \n",
       "6          2   15          2       2            1     6         1     1  \n",
       "7          2   14          1       2            0     6         1     1  \n",
       "8          2   16          2       2            0     6         2     1  \n",
       "9          2   15          2       3            5     6         1     2  \n",
       "10         1   13          1       1            1     6         1     1  \n",
       "11         1   16          2       3            5     2         1     1  \n",
       "12         1   17          1       3            5     1         1     1  \n",
       "13         1   21          2       2            2     6         1     1  \n",
       "14         1   15          1       2            1     6         1     1  \n",
       "15         1   15          1       2            5     6         1     1  \n",
       "16         1   14          1       1            1     2         2     1  \n",
       "17         1   55          4       1            1     6         2     1  \n",
       "18         1   25          3       2            2     1         1     1  \n",
       "19         2   27          1       2            2     6         1     1  \n",
       "20         1   22          3       2            5     6         0     2  \n",
       "21         2   20          2       3            5     6         1     1  \n",
       "22         1   23          2       2            2     6         1     1  \n",
       "23         2   18          2       2            2     6         1     1  \n",
       "24         0   18          2       1            2     6         1     1  \n",
       "25         1   20          3       2            2     1         1     1  \n",
       "26         1   49          3       2            3     6         0     1  \n",
       "27         1   29          4       3            3     6         1     1  \n",
       "28         1   32          3       2            1     6         2     3  \n",
       "29         1   21          3       2            1     1         2     2  \n",
       "...      ...  ...        ...     ...          ...   ...       ...   ...  \n",
       "4154       1   25          2       1            2     1         1     1  \n",
       "4155       1   16          2       2            3     2         6     1  \n",
       "4156       1   23          3       1            2     6         1     1  \n",
       "4157       2   15          1       2            2     6         1     1  \n",
       "4158       1   19          2       3            5     6         1     1  \n",
       "4159       2   20          2       1            1     7         7     1  \n",
       "4160       2   19          2       2            1     7         7     1  \n",
       "4161       2   32          4       1            1     6         2     1  \n",
       "4162       1   49          4       1            1     6         7     1  \n",
       "4163       2   17          2       2            4     2         3     1  \n",
       "4164       1   19          2       1            2     6         1     1  \n",
       "4165       2   23          2       2            1     6         1     1  \n",
       "4166       1   20          2       3            3     6         1     1  \n",
       "4167       1   19          0       0            0     0         0     1  \n",
       "4168       2   24          4       1            2     7         1     1  \n",
       "4169       2   30          4       1            1     6         1     1  \n",
       "4170       1   61          4       1            3     6         1     2  \n",
       "4171       2   19          3       2            3     6         1     1  \n",
       "4172       2   16          2       2            2     6         1     1  \n",
       "4173       2   18          2       2            1     6         2     1  \n",
       "4174       1   17          2       3            4     6         1     1  \n",
       "4175       1   21          4       1            3     6         1     1  \n",
       "4176       2   17          1       1            3     6         2     1  \n",
       "4177       1   51          2       2            4     6         2     1  \n",
       "4178       1   25          3       1            2     6         1     1  \n",
       "4179       1   18          2       1            1     6         2     1  \n",
       "4180       1   18          2       2            1     3         2     1  \n",
       "4181       2   22          2       1            1     6         1     1  \n",
       "4182       1   16          1       2            5     1         1     1  \n",
       "4183       1   22          3       2            4     6         1     1  \n",
       "\n",
       "[4184 rows x 56 columns]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppose that, instead of us giving you this data in a file, you were actually conducting a survey to gather this data yourself. From an ethics/privacy point of view, what are three things you might consider when attempting to gather this data?\n",
    "> When working with sensitive data like sexual orientation or gender identity, we need to consider how this data could be used if it fell into the wrong hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. We would want to consider if we could add an extra level of protection for potentially sensitive data. \n",
    "2. We would want to consider if we really even need to collect the sensitive data in the first place. \n",
    "3. We would want to consider if we should bring on an ethics expert to help analyze results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Explore the data.\n",
    "\n",
    "### 4. Conduct exploratory data analysis on this dataset.\n",
    "> If you haven't already, be sure to check out the codebook in the repo, as that will help in your EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engnat        -0.051152\n",
       "race          -0.046249\n",
       "age           -0.037029\n",
       "Q20           -0.031120\n",
       "Q10           -0.030222\n",
       "Q6            -0.026605\n",
       "Q24           -0.024519\n",
       "Q18           -0.022380\n",
       "Q22           -0.021202\n",
       "Q23           -0.019451\n",
       "education     -0.018760\n",
       "testelapse    -0.018266\n",
       "Q14           -0.017737\n",
       "Q8            -0.017541\n",
       "introelapse   -0.017293\n",
       "Q30           -0.016847\n",
       "Q2            -0.014258\n",
       "Q40           -0.013139\n",
       "Q12           -0.008176\n",
       "Q32           -0.005449\n",
       "gender        -0.004415\n",
       "Q21           -0.004328\n",
       "Q36           -0.002710\n",
       "Q41           -0.001979\n",
       "Q16            0.000540\n",
       "fromgoogle     0.005451\n",
       "Q42            0.009638\n",
       "religion       0.011911\n",
       "Q34            0.012736\n",
       "Q28            0.013155\n",
       "Q19            0.013284\n",
       "Q11            0.015954\n",
       "Q15            0.016632\n",
       "Q31            0.019128\n",
       "Q13            0.021554\n",
       "Q39            0.022652\n",
       "Q37            0.025436\n",
       "Q38            0.025876\n",
       "Q44            0.027689\n",
       "Q26            0.027984\n",
       "Q9             0.028658\n",
       "Q33            0.030640\n",
       "Q43            0.034401\n",
       "Q4             0.036163\n",
       "Q5             0.039186\n",
       "Q25            0.041948\n",
       "Q7             0.043052\n",
       "Q29            0.043592\n",
       "Q27            0.047491\n",
       "Q17            0.055071\n",
       "Q1             0.058110\n",
       "Q3             0.061920\n",
       "orientation    0.079642\n",
       "Q35            0.097932\n",
       "hand           1.000000\n",
       "Name: hand, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr().sort_values(by='hand').hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       1\n",
       "2       5\n",
       "3       1\n",
       "4       5\n",
       "5       5\n",
       "6       5\n",
       "7       5\n",
       "8       5\n",
       "9       1\n",
       "10      5\n",
       "11      1\n",
       "12      1\n",
       "13      1\n",
       "14      5\n",
       "15      1\n",
       "16      5\n",
       "17      5\n",
       "18      1\n",
       "19      1\n",
       "20      1\n",
       "21      1\n",
       "22      1\n",
       "23      1\n",
       "24      1\n",
       "25      1\n",
       "26      1\n",
       "27      1\n",
       "28      5\n",
       "29      5\n",
       "       ..\n",
       "4154    1\n",
       "4155    1\n",
       "4156    1\n",
       "4157    1\n",
       "4158    1\n",
       "4159    5\n",
       "4160    5\n",
       "4161    5\n",
       "4162    5\n",
       "4163    1\n",
       "4164    1\n",
       "4165    5\n",
       "4166    1\n",
       "4167    5\n",
       "4168    1\n",
       "4169    5\n",
       "4170    1\n",
       "4171    1\n",
       "4172    1\n",
       "4173    5\n",
       "4174    1\n",
       "4175    1\n",
       "4176    1\n",
       "4177    1\n",
       "4178    1\n",
       "4179    5\n",
       "4180    5\n",
       "4181    5\n",
       "4182    1\n",
       "4183    1\n",
       "Name: is_straight, Length: 4184, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_straight'] = df.orientation.map(lambda x: 1 if x > 1 else 5)\n",
    "df.is_straight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engnat        -0.051152\n",
       "race          -0.046249\n",
       "age           -0.037029\n",
       "Q20           -0.031120\n",
       "Q10           -0.030222\n",
       "Q6            -0.026605\n",
       "Q24           -0.024519\n",
       "Q18           -0.022380\n",
       "Q22           -0.021202\n",
       "Q23           -0.019451\n",
       "education     -0.018760\n",
       "testelapse    -0.018266\n",
       "Q14           -0.017737\n",
       "Q8            -0.017541\n",
       "introelapse   -0.017293\n",
       "Q30           -0.016847\n",
       "Q2            -0.014258\n",
       "Q40           -0.013139\n",
       "Q12           -0.008176\n",
       "Q32           -0.005449\n",
       "gender        -0.004415\n",
       "Q21           -0.004328\n",
       "Q36           -0.002710\n",
       "Q41           -0.001979\n",
       "Q16            0.000540\n",
       "fromgoogle     0.005451\n",
       "Q42            0.009638\n",
       "religion       0.011911\n",
       "Q34            0.012736\n",
       "Q28            0.013155\n",
       "Q19            0.013284\n",
       "Q11            0.015954\n",
       "Q15            0.016632\n",
       "Q31            0.019128\n",
       "Q13            0.021554\n",
       "Q39            0.022652\n",
       "Q37            0.025436\n",
       "Q38            0.025876\n",
       "Q44            0.027689\n",
       "Q26            0.027984\n",
       "Q9             0.028658\n",
       "Q33            0.030640\n",
       "Q43            0.034401\n",
       "Q4             0.036163\n",
       "Q5             0.039186\n",
       "Q25            0.041948\n",
       "Q7             0.043052\n",
       "Q29            0.043592\n",
       "Q27            0.047491\n",
       "Q17            0.055071\n",
       "Q1             0.058110\n",
       "Q3             0.061920\n",
       "is_straight    0.075347\n",
       "orientation    0.079642\n",
       "Q35            0.097932\n",
       "hand           1.000000\n",
       "Name: hand, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr().sort_values(by='hand').hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       4\n",
       "2       2\n",
       "3       5\n",
       "4       5\n",
       "5       5\n",
       "6       3\n",
       "7       1\n",
       "8       1\n",
       "9       5\n",
       "10      3\n",
       "11      4\n",
       "12      3\n",
       "13      5\n",
       "14      2\n",
       "15      1\n",
       "16      4\n",
       "17      4\n",
       "18      3\n",
       "19      1\n",
       "20      4\n",
       "21      4\n",
       "22      2\n",
       "23      5\n",
       "24      3\n",
       "25      5\n",
       "26      1\n",
       "27      1\n",
       "28      1\n",
       "29      5\n",
       "       ..\n",
       "4154    4\n",
       "4155    1\n",
       "4156    3\n",
       "4157    4\n",
       "4158    1\n",
       "4159    4\n",
       "4160    1\n",
       "4161    4\n",
       "4162    2\n",
       "4163    5\n",
       "4164    2\n",
       "4165    4\n",
       "4166    5\n",
       "4167    4\n",
       "4168    3\n",
       "4169    5\n",
       "4170    2\n",
       "4171    1\n",
       "4172    4\n",
       "4173    4\n",
       "4174    3\n",
       "4175    4\n",
       "4176    5\n",
       "4177    4\n",
       "4178    1\n",
       "4179    2\n",
       "4180    1\n",
       "4181    4\n",
       "4182    1\n",
       "4183    5\n",
       "Name: Q35, Length: 4184, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Q35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Model the data.\n",
    "\n",
    "### 5. Suppose I wanted to use Q1 - Q44 to predict whether or not the person is left-handed. Would this be a classification or regression problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I would say this should be classified as a regression problem because we are determining if there these qualities are a determinant of being left handed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed based on their responses to Q1 - Q44. Before doing that, however, you remember that it is often a good idea to standardize your variables. In general, why would we standardize our variables? Give an example of when we would standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In general we would want out variables to be standardized so that we could have everything be standardized. An example is this study where we want every response of the questions 1-44 to be the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Give an example of when we might not standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: An examplis of when we might not standardize our variables is when our variables are not useful to us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Based on your answers to 6 and 7, do you think we should standardize our predictor variables in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I think we should work with standardized variables,luckily they already are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed. What munging/cleaning do we need to do to our $y$ variable in order to explicitly answer this question? Do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We need to binarize our Y variable, see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_lefty'] = df.hand.map(lambda x: 1 if x == 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       1\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      1\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      1\n",
       "       ..\n",
       "4154    0\n",
       "4155    0\n",
       "4156    0\n",
       "4157    0\n",
       "4158    0\n",
       "4159    0\n",
       "4160    0\n",
       "4161    0\n",
       "4162    0\n",
       "4163    0\n",
       "4164    0\n",
       "4165    0\n",
       "4166    0\n",
       "4167    0\n",
       "4168    0\n",
       "4169    0\n",
       "4170    1\n",
       "4171    0\n",
       "4172    0\n",
       "4173    0\n",
       "4174    0\n",
       "4175    0\n",
       "4176    0\n",
       "4177    0\n",
       "4178    0\n",
       "4179    0\n",
       "4180    0\n",
       "4181    0\n",
       "4182    0\n",
       "4183    0\n",
       "Name: is_lefty, Length: 4184, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_lefty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Q35', 'is_straight', 'Q3']]\n",
    "y = df['is_lefty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.25288298, -1.16207631,  1.29061233],\n",
       "       [ 1.25288298,  0.86052869,  1.29061233],\n",
       "       [-0.69120572, -1.16207631,  1.29061233],\n",
       "       ...,\n",
       "       [ 1.25288298, -1.16207631,  1.29061233],\n",
       "       [-0.69120572, -1.16207631, -1.10588114],\n",
       "       [-0.69120572, -1.16207631,  1.29061233]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The professor for whom you work suggests that you set $k = 4$. In this specific case, why might this be a bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In this specific case it might be a bad idea because if k is set to 4 then we could end up having a tie and then classification would not be possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let's *(finally)* use $k$-nearest neighbors to predict whether or not a person is left-handed!\n",
    "\n",
    "> Be sure to create a train/test split with your data!\n",
    "\n",
    "> Create four separate models, one with $k = 3$, one with $k = 5$, one with $k = 15$, and one with $k = 25$.\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn5 = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn15 = KNeighborsClassifier(n_neighbors = 15)\n",
    "knn25 = KNeighborsClassifier(n_neighbors = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757096308449199"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn3, X_train_sc, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918451689371789"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn25, X_train_sc, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918434418882581"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train_sc, y_train)\n",
    "knn.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8924731182795699"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being good data scientists, we know that we might not run just one type of model. We might run many different models and see which is best.\n",
    "\n",
    "### 12. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, let's check the [documentation for logistic regression in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Is there default regularization? If so, what is it? If not, how do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: There is default regularization, which is L2 regularization with a constant of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, should we standardize our features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It doesn't matter because the coefficients will take that into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Let's use logistic regression to predict whether or not the person is left-handed.\n",
    "\n",
    "\n",
    "> Be sure to use the same train/test split with your data as with your $k$-NN model above!\n",
    "\n",
    "> Create four separate models, one with LASSO and $\\alpha = 1$, one with LASSO and $\\alpha = 10$, one with Ridge and $\\alpha = 1$, and one with Ridge and $\\alpha = 10$. *(Hint: Be careful with how you specify $\\alpha$ in your model!)*\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso1 = LogisticRegression(penalty = 'l1', C = 1.0)\n",
    "lasso10 = LogisticRegression(penalty = 'l1', C = 0.1)\n",
    "ridge1 = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "ridge10 = LogisticRegression(penalty = 'l2', C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\taylo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model1 = lasso1.fit(X_train, y_train)\n",
    "model2 = lasso10.fit(X_train, y_train)\n",
    "model3 = ridge1.fit(X_train, y_train)\n",
    "model4 = ridge10.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the model(s).\n",
    "\n",
    "### 15. Before calculating any score on your data, take a step back. Think about your $X$ variable and your $Y$ variable. Do you think your $X$ variables will do a good job of predicting your $Y$ variable? Why or why not? What impact do you think this will have on your scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I do not think the X variables will do a good job of predicting my Y variable because logically I would be surprised if there was any type of link, and all of the correlation numbers were so little. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Using accuracy as your metric, evaluate all eight of your models on both the training and testing sets. Put your scores below. (If you want to be fancy and generate a table in Markdown, there's a [Markdown table generator site linked here](https://www.tablesgenerator.com/markdown_tables#).)\n",
    "- Note: Your answers here might look a little weird. You didn't do anything wrong; that's to be expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn3.fit(X_train_sc, y_train)\n",
    "[knn3.score(X_train_sc, y_train), knn3.score(X_test_sc, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn5.fit(X_train_sc, y_train)\n",
    "[knn5.score(X_train_sc, y_train), knn5.score(X_test_sc, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn15.fit(X_train_sc, y_train)\n",
    "[knn15.score(X_train_sc, y_train), knn15.score(X_test_sc, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn25.fit(X_train_sc, y_train)\n",
    "[knn25.score(X_train_sc, y_train), knn25.score(X_test_sc, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model1.score(X_train, y_train), model1.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model2.score(X_train, y_train), model2.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model3.score(X_train, y_train), model3.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8918434418882581, 0.8924731182795699]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model4.score(X_train, y_train), model4.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. In which of your $k$-NN models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: There is not evidence of overfitting in any of my KNN models as all my scores are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Broadly speaking, how does the value of $k$ in $k$-NN affect the bias-variance tradeoff? (i.e. As $k$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Broadly speaking as the value of K increases the bias decreases and variance increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. If you have a $k$-NN model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1. Increase K\n",
    "2. Increase proportion of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. In which of your logistic regression models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: None of them because all have the same score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Broadly speaking, how does the value of $C$ in logistic regression affect the bias-variance tradeoff? (i.e. As $C$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Broadly speaking as C increases bias decreases ans variance increases as we are normalizing more. \n",
    "As C decreases bias increases and variance increases as we are normalizing more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. For your logistic regression models, play around with the regularization hyperparameter, $C$. As you vary $C$, what happens to the fit and coefficients in the model? What do you think this means in the context of this specific problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Changing our C we get the same model, shows that our X variables are not very useful in predicting left handedness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. If you have a logistic regression model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. Remove features from the model.\n",
    "2. Collect more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Answer the problem.\n",
    "\n",
    "### 24. Suppose you want to understand which psychological features are most important in determining left-handedness. Would you rather use $k$-NN or logistic regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I would rather use logistic regression because I could use the coeffiecients to see the impact on left handedness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Select your logistic regression model that utilized LASSO regularization with $\\alpha = 1$. Interpret the coefficient for `Q1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: To make sure my model fit I only selected 3 coefficients that had the strongest scores and those from Q1 were not any of them.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02319566, -0.00067903, -0.00151462]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. If you have to select one model overall to be your *best* model, which model would you select? Why?\n",
    "- Usually in the \"real world,\" you'll fit many types of models but ultimately need to pick only one! (For example, a client may not understand what it means to have multiple models, or if you're using an algorithm to make a decision, it's probably pretty challenging to use two or more algorithms simultaneously.) It's not always an easy choice, but you'll have to make it soon enough. Pick a model and defend why you picked this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It would be the one with the higher test score, which in this case could be all of them and in this case I would choose my Lasso10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Circle back to the three specific and conclusively answerable questions you came up with in Q1. Answer one of these for the professor based on the model you selected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: So I did kind of change my questions from 1 once I found out there were better coefficients I could use, but changing the questions a little would be these...\n",
    "1. Does an association exist between having thrown knives, axes or other sharp thingss and being left handed? NO\n",
    "2. Does an association exist between sexual orientation and being left handed? NO\n",
    "3. Does an association exist between having taking apart machines to see how they work and being left handed? NO\n",
    "\n",
    "For all three of these the coefficient score is so low it means there is no real link between any of them and being left handed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS:\n",
    "Looking for more to do? Probably not - you're busy! But if you want to, consider exploring the following. (They could make for a blog post!)\n",
    "- Create a visual plot comparing training and test metrics for various values of $k$ and various regularization schemes in logistic regression.\n",
    "- Rather than just evaluating models based on accuracy, consider using sensitivity, specificity, etc.\n",
    "- In the context of predicting left-handedness, why are unbalanced classes concerning? If you were to re-do this process given those concerns, what changes might you make?\n",
    "- Fit and evaluate a generalized linear model other than logistic regression (e.g. Poisson regression).\n",
    "- Suppose this data were in a `SQL` database named `data` and a table named `inventory`. What `SQL` query would return the count of people who were right-handed, left-handed, both, or missing with their class labels of 1, 2, 3, and 0, respectively? (You can assume you've already logged into the database.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
